$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

name: compress_pytorch_model
display_name: Compress PyTorch Model
tags:
  author: sfta-team

inputs:
  run_name:
    type: string
  pytorch_model_dir:
    type: uri_folder
  quantization_method:
    type: string
    description: "Quantization method to use"
    enum:
      - awq
  quantization_precision:
    type: string
    description: "Quantization precision to use"
    enum:
      - int4
      - int8
      - fp16
  use_mlflow:
    type: boolean
    optional: true

outputs:
  model_dir:
    type: uri_folder

code: ../../../../../

environment: azureml:gpu-quantization-environment@latest

command: >-
  python src/run_model_compression.py
  --run-name ${{inputs.run_name}}
  --pytorch-model-dir ${{inputs.pytorch_model_dir}}
  --quantization-method ${{inputs.quantization_method}}
  --quantization-precision ${{inputs.quantization_precision}}
  $[[--use-mlflow ${{inputs.use_mlflow}}]]
  --model-dir ${{outputs.model_dir}}