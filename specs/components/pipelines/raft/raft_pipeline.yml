$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json

display_name: raft_pipeline
experiment_name: raft_experiment
description: End-to-end pipeline for SLM-based RAFT scenario
type: pipeline

settings:
  default_datastore: azureml:workspaceblobstore
  default_compute: azureml:cpu-compute
  continue_on_step_failure: false

inputs:
  finetuning_node_compute: azureml:gpu-fintetuning-compute
  quantization_node_compute: azureml:gpu-quantization-compute
  deployment_node_compute: azureml:cpu-deployment-compute
  finetuning_run_name: raft_finetuning_demo
  compression_run_name: raft_finetuning_demo
  train_file:
    type: uri_file
    path: "../../../../data/processed/raft_sample_data-files/raft_sample_data-ft.train.jsonl"
  base_model_id: microsoft/Phi-3-mini-128k-instruct
  quantization_mode: "qat"
  flash_attention: true
  peft_approach: dora
  finetune_approach: sfttrainer
  optimizer: adamw_torch_fused
  max_seq_length: 4096
  num_train_epochs: 1
  learning_rate: 0.0001
  per_device_train_batch_size: 3
  per_device_eval_batch_size: 3
  gradient_accumulation_steps: 6
  gradient_checkpointing: true
  logging_steps: 10
  save_steps: 10
  eval_steps: 10
  use_mlflow: true
  quantization_method: awq
  quantization_precision: int4
  endpoint_type: online
  endpoint_name: raft-phi3-awq-int4
  

outputs:
  model_dir:
    type: uri_folder
  quantized_model_dir:
    type: uri_folder

jobs:
  finetune_model:
    type: command
    component: ../../commands/finetune/sft/component.yml
    compute: ${{parent.inputs.finetuning_node_compute}}
    inputs:
      run_name: ${{parent.inputs.finetuning_run_name}}
      train_file: ${{parent.inputs.train_file}}
      base_model_id: ${{parent.inputs.base_model_id}}
      quantization_mode: ${{parent.inputs.quantization_mode}}
      flash_attention: ${{parent.inputs.flash_attention}}
      peft_approach: ${{parent.inputs.peft_approach}}
      finetune_approach: ${{parent.inputs.finetune_approach}}
      optimizer: ${{parent.inputs.optimizer}}
      max_seq_length: ${{parent.inputs.max_seq_length}}
      num_train_epochs: ${{parent.inputs.num_train_epochs}}
      learning_rate: ${{parent.inputs.learning_rate}}
      per_device_train_batch_size: ${{parent.inputs.per_device_train_batch_size}}
      per_device_eval_batch_size: ${{parent.inputs.per_device_eval_batch_size}}
      gradient_accumulation_steps: ${{parent.inputs.gradient_accumulation_steps}}
      gradient_checkpointing: ${{parent.inputs.gradient_checkpointing}}
      logging_steps: ${{parent.inputs.logging_steps}}
      save_steps: ${{parent.inputs.save_steps}}
      eval_steps: ${{parent.inputs.eval_steps}}
      use_mlflow: ${{parent.inputs.use_mlflow}}
    outputs:
      model_dir: ${{parent.outputs.model_dir}}

  quantize_model:
    type: command
    component: ../../commands/model_compression/onnx/component.yml
    compute: ${{parent.inputs.quantization_node_compute}}
    inputs:
      run_name: ${{parent.inputs.compression_run_name}}
      pytorch_model_dir: ${{parent.jobs.finetune_model.outputs.model_dir}}
      quantization_method: ${{parent.inputs.quantization_method}}
      quantization_precision: ${{parent.inputs.quantization_precision}}
      use_mlflow: ${{parent.inputs.use_mlflow}}
    outputs:
      model_dir: ${{parent.outputs.quantized_model_dir}}
  
  deploy_quantized_model:
    type: command
    component: ../../commands/deployment/finetuned/managed_compute/component.yml
    compute: ${{parent.inputs.deployment_node_compute}}
    inputs:
      model_dir: ${{parent.jobs.quantize_model.outputs.model_dir}}
      base_model_id: ${{parent.inputs.base_model_id}}
      endpoint_type: ${{parent.inputs.endpoint_type}}
      endpoint_name: ${{parent.inputs.endpoint_name}}
