{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807355d8-01a2-4082-9dd9-822e026348a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires Python 3.10 or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23cde469-d817-462c-8f3c-be5636bde42f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0c5834-94e4-448f-b113-ee2cd94d93c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install azure-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "405773d7-9a83-4dee-a10f-4985bc024aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "import torch\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "import time\n",
    "import onnxruntime_genai as og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63aee57f-df19-4dac-9441-a9ff3cf5bd08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"finetuning_experiment\"\n",
    "output_name = \"model_dir\"\n",
    "output_directory = \"./models\"\n",
    "finetuned_model_path = f\"{output_directory}/named-outputs/model_dir/merged\"\n",
    "finetuned_quantized_awq_model_path = f\"{output_directory}/named-outputs/model_dir/merged-awq\"\n",
    "finetuned_quantized_onnx_model_path = f\"{output_directory}/named-outputs/model_dir/merged-onnx\"\n",
    "finetuned_quantized_awq_to_onnx_model_path = f\"{output_directory}/named-outputs/model_dir/merged-awq-onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be8612df-2ceb-49ef-a6e2-c125ce2b72b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_output_onnx(model, tokenizer, prompt):\n",
    "    tokenizer_stream = tokenizer.create_stream()\n",
    "    input_tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    search_options = {\n",
    "        \"max_length\": 2048, \n",
    "        \"temperature\": 0.0, \n",
    "        \"do_sample\": False\n",
    "    }\n",
    "\n",
    "    params = og.GeneratorParams(model)\n",
    "    params.set_search_options(**search_options)\n",
    "    params.input_ids = input_tokens\n",
    "    generator = og.Generator(model, params)\n",
    "    \n",
    "    started_timestamp = time.time()\n",
    "\n",
    "    first = True\n",
    "    new_tokens = []\n",
    "    new_tokens_decoded = []\n",
    "\n",
    "    while not generator.is_done():\n",
    "        generator.compute_logits()\n",
    "        generator.generate_next_token()\n",
    "        if first:\n",
    "            first_token_timestamp = time.time()\n",
    "            first = False\n",
    "        new_token = generator.get_next_tokens()[0]\n",
    "        new_token_decoded = tokenizer_stream.decode(new_token)\n",
    "        print(new_token_decoded, end='', flush=True)\n",
    "        new_tokens.append(new_token)\n",
    "        new_tokens_decoded.append(new_token_decoded)\n",
    "    \n",
    "    prompt_length = len(input_tokens)\n",
    "    new_tones_length = len(new_tokens)\n",
    "    first_token_time = first_token_timestamp - started_timestamp\n",
    "    run_time = time.time() - first_token_timestamp\n",
    "    prompt_tokens_per_second = len(input_tokens)/first_token_time\n",
    "    new_tokens_per_second = len(new_tokens)/run_time\n",
    "    \n",
    "    generated_output = \"\".join(new_tokens_decoded)\n",
    "    \n",
    "    del generator\n",
    "        \n",
    "    return generated_output, first_token_time, run_time, prompt_tokens_per_second, new_tokens_per_second\n",
    "\n",
    "def get_formatted_context(chunks):\n",
    "    BEGIN = \"<DOCUMENT>\"\n",
    "    END = \"</DOCUMENT>\"\n",
    "    NEW_LINE = \"\\n\"\n",
    "\n",
    "    context = [f\"{BEGIN}{chunk}{END}{NEW_LINE}\" for chunk in chunks]\n",
    "    return \"\".join(context)\n",
    "\n",
    "\n",
    "def get_chat_template_input(meta_prompt, context, query):\n",
    "    messages = [\n",
    "        {\"content\": meta_prompt, \"role\": \"system\"},\n",
    "        {\"content\": f\"{context}{query}.\", \"role\": \"user\"}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "meta_prompt = \"The following is a conversation with an AI assistant. The assistant is helpful, clever, friendly and gives concise and accurate answers.\"\n",
    "chunks = [\n",
    "    \"Various. Verify the error codes in the balance\\u2019s manual.MAINTENANCE MANUAL FOR LABORATORY EQUIPMENT\\n29BASIC DEFINITIONS\\nASTM. American Society of Testing and Materials.\",\n",
    "    \"Preventive maintenance\\nFrequency: Quarterly1. Verify the stability of the lamp. Use the calibration plate, \\nconducting readings with intervals of 30 minutes with the same plate. Compare readings. There must be no diff erences.\",\n",
    "    \"2. A measuring device known as \\u201cload cell\\u201d produces an \\nexit signal corresponding to the load\\u2019s force in the form of changes in the voltage or frequency. 3. A digital analogous electronic circuit shows the fi  nal \\nresult of the weight digitally. Laboratory balances operate according to the principle \\nof compensation of the electromagnetic force applicable to displacements or torques. The combination of their mechanical components and automatic reading systems provides weight measurements at defi  ned levels of accuracy \\ndepending on the model. Principle. The mobile parts (weighing plate, support \\ncolumn [a], bobbin, position and load indicator [G] -the object in the process of being weighed-) are maintained in equilibrium by a compensation force [F] equal to the weight. The compensation force is generated by an electrical current through a bobbin in the air gap of a cylindrical electromagnet. The force F is calculated with the equation [F = I x L x B] where: I = electrical intensity, L = total length of the wire of the coil and B = magnetic fl  ow intensity in the \\nelectromagnet\\u2019s air gap.With any change in the load (weight\\/mass), the mobile \\nmechanical system responds by moving vertically a fraction of distance. Detected by a photosensor [e], an electrical signal is sent to the servo-amplifi  er [f]. This changes the \\nfl ow of electrical current passing through the bobbin of the \\nmagnet [c] in such a manner that the mobile system returns to the balanced position upon adjusting of the magnetic fl ow in the electromagnet. Consequently, the weight of \\nthe mass [G] can be measured indirectly at the start of the electrical current fl  ow, which passes through the circuit \\nmeasuring the voltage [V] by means of a precision resistor [R], [V = I x R]. To date, many systems developed use the electronic system for carrying out very exact measurements of mass and weight. The following diagram explains how electronic balances function. Transfer\\nMechanism\\nLoad Cell\\nScreen and\\nSignal ProcessorPFigure 12. Components of electronic balances  \\nG\\nb\\na\\ne\\nfc dR V=I*R\\nIFigure 13. Compensation force principle  \\nMAINTENANCE MANUAL FOR LABORATORY EQUIPMENT\\n25The signal processing system\\nThe signal processing system is composed of the circuit which \\ntransf orms the electrical signal emitted by the transducer \\ninto numerical data which can be read on a screen. The signal process comprises the following functions:1. Tare setting. This setting is used to adjust the reading \\nvalue at zero with any load within the balance\\u2019s capacity range. It is controlled by a button generally located on the front part of the balance. It is commonly used for taring the weighing container. 2. Repeatability setting control. During a reading, weighed \\nvalues are averaged within a predefi  ned period of time. This function is very useful when weighing operations need to be carried out in unstable conditions, e.g. in the presence of air currents or vibrations. This control defi nes the time period allowed for a result to lie within \\npreset limits for it to be considered stable. In addition, it can be adjusted to suit a particular application.\",\n",
    "    \"Any spill must be cleaned immediately to avoid corrosion \\nor contamination. Use 70% ethanol to disinfect the pan of the balance. Very important:  Never lubricate a balance unless the \\nmanufacturer has expressly indicated it. Any substance interfering with the mechanism of the balance retards its response or defi  nitely alters the measurement process. Note:  In general, the manufacturer or the specialized \\ninstallation representative carries out the maintenance of the balances, according to procedures which vary depending on the type and model. 1 Guidelines for calibration in laboratories, Drinking Water Inspectorate by \\nLGC (Teddington) Ltd., December 2000. CapacityResolution\\n100 g 10 g 1 g 100 mg 10 mg 1 mg 0.1 mg \\u00980.01 mg \\nUp to 200 g \\u2013 \\u2013 \\u2013 M1 M1 F2 F1 F2\\n200 g to 1 kg \\u2013 \\u2013 M1 M1 F2 F1\\/E2 E2 E2\\n1 to 30 kg M2 M2 M1 F2 E2 E2 E2 \\u2013\\n30 to 100 kg M2 M1 F2 F1 E2 \\u2013 \\u2013 \\u2013\\nMore than \\n100 kgM2 M1\\/F2 F1 E2 \\u2013 \\u2013 \\u2013 \\u2013Table of standard weights\\u2019 use according to the balance\\u2019s capacity  CHAPTER 4  BALANCES\\n28FUNCTIONAL ERROR PROBABLE CAUSE\\nReadings not reproducible (hysteresis). The measurement cell is dirty.\",\n",
    "    \"Check the lubrication state of elements such as for \\nO-rings as the manufacturer recommends. Always use lubricants according to the manufacturer\\u2019s instructions (frequency and type of lubricants). In recently manufactured centrifuges, there are sealed ball bearings which do not require lubrication. 5.\",\n",
    "    \"Remove the cover of the boiling tank.3. Visually verify if the interior walls or the immersion \\nresistors show solid deposits or sediments. The quantity of deposits present depends on the quality of water fed to the distiller. If there is an accumulation of sediments, it must be cleaned to avoid damaging the resistors\\n1. 4.\"\n",
    "]\n",
    "context = get_formatted_context(chunks)\n",
    "query = \"What does ASTM stand for?\"\n",
    "messages = get_chat_template_input(meta_prompt, context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f353a7-2f10-4969-b3bd-e71b788a28e4",
   "metadata": {},
   "source": [
    "# Substitute \"merged\" with new transformer model trained with new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70985703-1105-43a5-9ce1-235a5a20b256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory):\n",
    "    if not os.listdir(output_directory):   \n",
    "        try:\n",
    "            credential = DefaultAzureCredential()\n",
    "            credential.get_token(\"https://management.azure.com/.default\")\n",
    "        except Exception as ex:\n",
    "            # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "            # This will open a browser page for\n",
    "            credential = InteractiveBrowserCredential()\n",
    "            \n",
    "        ml_client = MLClient.from_config(\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        \n",
    "        jobs = ml_client.jobs.list()\n",
    "\n",
    "        filtered_jobs = [job for job in jobs if job.experiment_name == experiment_name]\n",
    "        print(filtered_jobs[-1])\n",
    "        \n",
    "        job_name = filtered_jobs[-1].name\n",
    "        ml_client.jobs.download(name=job_name, output_name=output_name, download_path=output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bce649a5-3c0d-43ce-81fe-49113eb04786",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 13.36s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "    finetuned_model_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c62b1d62-966f-458a-a895-fea592e7149e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer the question about what ASTM stands for, I will follow these steps:\n",
      "\n",
      "1. Identify the acronym ASTM in the context provided.\n",
      "2. Since the context does not provide a definition for ASTM, I will reference my general knowledge.\n",
      "3. Upon research or knowledge, I find that ASTM stands for \"American Society for Testing and Materials.\"\n",
      "\n",
      "Now, I will compile the information accordingly.\n",
      "\n",
      "##begin_quote## ASTM stands for American Society for Testing and Materials. ##end_quote##\n",
      "\n",
      "<ANSWER>: American Society for Testing and Materials\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "  messages,\n",
    "  tokenize=True,\n",
    "  add_generation_prompt=True,\n",
    "  return_tensors=\"pt\",\n",
    "  return_dict=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=512)\n",
    "print(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2d1c849-8cd6-4dfd-8f1a-64b8ed3c2b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ: 100%|██████████| 32/32 [09:36<00:00, 18.00s/it]\n",
      "Note that `shard_checkpoint` is deprecated and will be removed in v4.44. We recommend you using split_torch_state_dict_into_shards from huggingface_hub library\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/named-outputs/model_dir/merged-awq/tokenizer_config.json',\n",
       " './models/named-outputs/model_dir/merged-awq/special_tokens_map.json',\n",
       " './models/named-outputs/model_dir/merged-awq/tokenizer.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_config = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\"\n",
    "}\n",
    "\n",
    "model.quantize(tokenizer, quant_config = quant_config)\n",
    "\n",
    "model.save_quantized(finetuned_quantized_awq_model_path) # save_quantized\n",
    "tokenizer.save_pretrained(finetuned_quantized_awq_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d69a238f-4c5c-438b-b3fb-f889127394ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncpu_memory_usage = sys.getsizeof(model)\\nprint(f\"CPU Memory occupied by the model: {cpu_memory_usage / (1024 ** 2)} MB\")\\n\\nmodel = model.to(\\'cuda\\')\\n\\ngpu_memory_usage = torch.cuda.memory_allocated()\\nprint(f\"GPU Memory occupied by the model: {gpu_memory_usage / (1024 ** 2)} MB\")\\n\\ngpu_memory_reserved = torch.cuda.memory_reserved()\\nprint(f\"GPU Memory reserved: {gpu_memory_reserved / (1024 ** 2)} MB\")\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cpu_memory_usage = sys.getsizeof(model)\n",
    "print(f\"CPU Memory occupied by the model: {cpu_memory_usage / (1024 ** 2)} MB\")\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "gpu_memory_usage = torch.cuda.memory_allocated()\n",
    "print(f\"GPU Memory occupied by the model: {gpu_memory_usage / (1024 ** 2)} MB\")\n",
    "\n",
    "gpu_memory_reserved = torch.cuda.memory_reserved()\n",
    "print(f\"GPU Memory reserved: {gpu_memory_reserved / (1024 ** 2)} MB\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "049d6ae0-25ab-4e5c-bf00-3c6b5bd8c006",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out what ASTM stands for, I will follow these steps:\n",
      "\n",
      "1. Identify the acronym \"ASTM\" in the context provided.\n",
      "2. Look for any relevant definitions or explanations of the acronym in the context.\n",
      "3. Summarize the information to provide a clear answer.\n",
      "\n",
      "Now, examining the context, I see that it mentions \"ASTM\" but does not provide a definition. Therefore, I will rely on my general knowledge to answer the question.\n",
      "\n",
      "ASTM stands for:\n",
      "- AST: Association for Small Tank Manufacturers\n",
      "- M: Meter (Inducing Measurement)\n",
      "- T: Time (Dependent Measurement)\n",
      "- E: Electrical (Direct or Alternative Currents)\n",
      "- R: Radio waves\n",
      "- L: Light (Visible Spectrum)\n",
      "\n",
      "So, ASTM is an acronym for various technical organizations associated with standards and measurements.\n",
      "\n",
      "<ANSWER>: ASTM stands for Association for Small Tank Manufacturers.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "  messages,\n",
    "  tokenize=True,\n",
    "  add_generation_prompt=True,\n",
    "  return_tensors=\"pt\",\n",
    "  return_dict=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "model.to(\"cuda\")\n",
    "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=512)\n",
    "print(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ece4af7-720f-41f4-8b84-058182fab611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_pt_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    finetuned_quantized_awq_model_path,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=False\n",
    ")\n",
    "\n",
    "loaded_pt_model = AutoModelForCausalLM.from_pretrained(\n",
    "    finetuned_quantized_awq_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False\n",
    ").eval()\n",
    "\n",
    "loaded_pt_model = loaded_pt_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd115979-21d1-4fef-964a-1a8a8cbdbe45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer the question \"What does ASTM stand for?\", we can follow these steps:\n",
      "\n",
      "1. Identify the acronym ASTM in the context provided.\n",
      "2. Look for the definition or meaning associated with ASTM.\n",
      "3. Since the context only states \"ASTM,\" we need to understand its meaning based on common knowledge.\n",
      "\n",
      "ASTM stands for \"American Society for Testing and Materials.\" This is a common organization known for setting standards and conducting tests on materials across various industries.\n",
      "\n",
      "Final answer:\n",
      "<ANSWER>: ASTM stands for American Society for Testing and Materials.\n"
     ]
    }
   ],
   "source": [
    "inputs = loaded_pt_tokenizer.apply_chat_template(\n",
    "  messages,\n",
    "  tokenize=True,\n",
    "  add_generation_prompt=True,\n",
    "  return_tensors=\"pt\",\n",
    "  return_dict=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = loaded_pt_model.generate(**inputs, do_sample=True, max_new_tokens=512)\n",
    "print(loaded_pt_tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969b951-9e15-4581-8caa-baf09f76c059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c49be87-8c2d-4e3c-ac18-746e206b5d66",
   "metadata": {},
   "source": [
    "# TRY TO RUN IT ON CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20dbb299-b6bd-41f7-a8c4-dddbf31ba3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ./models/named-outputs/model_dir/merged size: 2.12 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    # Walk through all directories and files in the folder\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for f in filenames:\n",
    "            # Get the path of each file\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            # Add the size of each file to the total size\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "#finetuned_model_path\n",
    "#finetuned_quantized_awq_model_path\n",
    "#finetuned_quantized_onnx_model_path\n",
    "#finetuned_quantized_awq_to_onnx_model_path\n",
    "\n",
    "# Calculate the size in bytes\n",
    "folder_size = get_folder_size(finetuned_quantized_awq_model_path)\n",
    "\n",
    "# Convert the size to more readable formats (e.g., MB, GB)\n",
    "size_in_mb = folder_size / (1024 * 1024)\n",
    "size_in_gb = folder_size / (1024 * 1024 * 1024)\n",
    "\n",
    "print(f\"Folder {finetuned_model_path} size: {size_in_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3308db3-0798-4b9f-9c8b-c72153d59dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW JUST CONVERT TO ONNX AND CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eab7ae-0835-4824-9ef0-2e3ba4e110b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install optimum[exporters]\n",
    "#!pip install onnxruntime-genai==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "897017fe-5d51-46d6-95be-a2a89ff75b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/named-outputs/model_dir/merged-awq\n",
      "./models/named-outputs/model_dir/merged-onnx\n",
      "./models/named-outputs/model_dir/merged-awq-onnx\n"
     ]
    }
   ],
   "source": [
    "print(finetuned_quantized_awq_model_path)\n",
    "print(finetuned_quantized_onnx_model_path)\n",
    "print(finetuned_quantized_awq_to_onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df793c3-c33b-4202-9a8d-fdc6f0d430c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-10-08 11:45:15.871162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-08 11:45:15.895124: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-08 11:45:15.902834: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-08 11:45:15.921275: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-08 11:45:17.174922: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:48<00:00, 27.13s/it]\n",
      "The task `text-generation` was manually specified, and past key values will not be reused in the decoding. if needed, please pass `--task text-generation-with-past` to export using the past key values.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "***** Exporting submodel 1/1: Phi3ForCausalLM *****\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/bin/optimum-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/optimum/commands/optimum_cli.py\", line 208, in main\n",
      "    service.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/optimum/commands/export/onnx.py\", line 265, in run\n",
      "    main_export(\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/optimum/exporters/onnx/__main__.py\", line 374, in main_export\n",
      "    onnx_export_from_model(\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/optimum/exporters/onnx/convert.py\", line 1171, in onnx_export_from_model\n",
      "    _, onnx_outputs = export_models(\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/optimum/exporters/onnx/convert.py\", line 776, in export_models\n",
      "    export(\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/optimum/exporters/onnx/convert.py\", line 862, in export\n",
      "    raise MinimumVersionError(\n",
      "optimum.exporters.error_utils.MinimumVersionError: The current version of Transformers does not allow for the export of the model. Minimum required is 4.41.0, got: 4.36.2\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --task text-generation --framework pt --trust-remote-code --model ./models/named-outputs/model_dir/merged/ ./models/named-outputs/model_dir/merged-onnx-optimum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9d1c5-c2d3-462e-983f-71cf42ac20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs GPU to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56696c8-6c48-4592-91e3-d63dcac4e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --task text-generation --framework pt --trust-remote-code --model ./models/named-outputs/model_dir/merged-awq/ ./models/named-outputs/model_dir/merged-awq-onnx-optimum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c6d0b-80db-4242-b5e1-f8b65426ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW QUANTIZE WITH ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9893c-8194-4d73-8483-6296dd9020db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Valid precision + execution provider combinations are: FP32 CPU, FP32 CUDA, FP16 CUDA, FP16 DML, INT4 CPU, INT4 CUDA, INT4 DML\n",
      "Extra options: {}\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1067: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "GroupQueryAttention (GQA) is used in this model.\n",
      "Unpacking and repacking layer 0\n",
      "Unpacking and repacking layer 1\n",
      "Unpacking and repacking layer 2\n",
      "Unpacking and repacking layer 3\n",
      "Unpacking and repacking layer 4\n",
      "Unpacking and repacking layer 5\n",
      "Unpacking and repacking layer 6\n",
      "Unpacking and repacking layer 7\n",
      "Unpacking and repacking layer 8\n",
      "Unpacking and repacking layer 9\n",
      "Unpacking and repacking layer 10\n",
      "Unpacking and repacking layer 11\n",
      "Unpacking and repacking layer 12\n",
      "Unpacking and repacking layer 13\n",
      "Unpacking and repacking layer 14\n",
      "Unpacking and repacking layer 15\n",
      "Unpacking and repacking layer 16\n",
      "Unpacking and repacking layer 17\n",
      "Unpacking and repacking layer 18\n",
      "Unpacking and repacking layer 19\n",
      "Unpacking and repacking layer 20\n",
      "Unpacking and repacking layer 21\n",
      "Unpacking and repacking layer 22\n",
      "Unpacking and repacking layer 23\n",
      "Unpacking and repacking layer 24\n",
      "Unpacking and repacking layer 25\n",
      "Unpacking and repacking layer 26\n",
      "Unpacking and repacking layer 27\n",
      "Unpacking and repacking layer 28\n",
      "Unpacking and repacking layer 29\n",
      "Unpacking and repacking layer 30\n",
      "Unpacking and repacking layer 31\n",
      "Reading embedding layer\n",
      "Reading decoder layer 0\n",
      "Reading decoder layer 1\n",
      "Reading decoder layer 2\n",
      "Reading decoder layer 3\n",
      "Reading decoder layer 4\n",
      "Reading decoder layer 5\n",
      "Reading decoder layer 6\n",
      "Reading decoder layer 7\n",
      "Reading decoder layer 8\n",
      "Reading decoder layer 9\n",
      "Reading decoder layer 10\n",
      "Reading decoder layer 11\n",
      "Reading decoder layer 12\n",
      "Reading decoder layer 13\n",
      "Reading decoder layer 14\n",
      "Reading decoder layer 15\n",
      "Reading decoder layer 16\n",
      "Reading decoder layer 17\n",
      "Reading decoder layer 18\n",
      "Reading decoder layer 19\n",
      "Reading decoder layer 20\n",
      "Reading decoder layer 21\n",
      "Reading decoder layer 22\n",
      "Reading decoder layer 23\n",
      "Reading decoder layer 24\n",
      "Reading decoder layer 25\n",
      "Reading decoder layer 26\n",
      "Reading decoder layer 27\n",
      "Reading decoder layer 28\n",
      "Reading decoder layer 29\n",
      "Reading decoder layer 30\n",
      "Reading decoder layer 31\n",
      "Reading final norm\n",
      "Reading LM head\n",
      "Saving ONNX model in ./models/named-outputs/model_dir/merged-onnx\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:712: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Saving GenAI config in ./models/named-outputs/model_dir/merged-onnx\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/onnxruntime_genai/models/builder.py\", line 2615, in <module>\n",
      "    create_model(args.model_name, args.input, args.output, args.precision, args.execution_provider, args.cache_dir, **extra_options)\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/onnxruntime_genai/models/builder.py\", line 2519, in create_model\n",
      "    onnx_model.save_processing(hf_name, extra_kwargs, output_dir)\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/onnxruntime_genai/models/builder.py\", line 340, in save_processing\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_auth_token=True, trust_remote_code=True, **extra_kwargs)\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 787, in from_pretrained\n",
      "    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2028, in from_pretrained\n",
      "    return cls._from_pretrained(\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2260, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\", line 124, in __init__\n",
      "    super().__init__(\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 111, in __init__\n",
      "    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n",
      "Exception: data did not match any variant of untagged enum ModelWrapper at line 277209 column 3\n"
     ]
    }
   ],
   "source": [
    "!python -m onnxruntime_genai.models.builder -i \"./models/named-outputs/model_dir/merged-awq\" -o \"./models/named-outputs/model_dir/merged-onnx\" -p int4 -e cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70728eac-f84d-4d85-b37e-09a86a71d480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOW BENCHMARK:\n",
    "# https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/models/llama/benchmark_e2e.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c5e5c-b00a-4a0a-9114-38936b683b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
