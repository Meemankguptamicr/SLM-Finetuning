{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efaef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install azureml-widgets\n",
    "#%pip install mlflow\n",
    "#%pip install azure-ai-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770bc234",
   "metadata": {},
   "source": [
    "## 1. Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ca723a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml.entities import Environment, BuildContext, AmlCompute, ComputeInstance\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "dotenv_path = Path(\"my.env\")\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37247b2f",
   "metadata": {},
   "source": [
    "## 2. Get a Handle to the Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a196fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    # This will open a browser page for\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381b6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "ml_client = MLClient.from_config(\n",
    "    credential=credential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8964cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "azureml_tracking_uri = ml_client.workspaces.get(\n",
    "    ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "mlflow.set_tracking_uri(azureml_tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15fb8f",
   "metadata": {},
   "source": [
    "## 3. Create the Compute Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e3d36f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute cluster.\n"
     ]
    }
   ],
   "source": [
    "# Compute for scoring\n",
    "compute_cluster = True\n",
    "compute_version = \"nc24ads-a100-v4\" #\"nc12s-v3\"\n",
    "compute_type = \"Standard_NC24ads_A100_v4\" #\"Standard_NC12s_v3\"\n",
    "\n",
    "if compute_cluster:\n",
    "    gpu_compute_name = f\"{compute_version}-cluster\"\n",
    "    try:\n",
    "        _ = ml_client.compute.get(gpu_compute_name)\n",
    "        print(\"Found existing compute cluster.\")\n",
    "    except ResourceNotFoundError:\n",
    "        print(\"Creating a new compute cluster...\")\n",
    "        compute_config = AmlCompute(\n",
    "            name=gpu_compute_name,\n",
    "            type=\"amlcompute\",\n",
    "            size=compute_type,\n",
    "            idle_time_before_scale_down=120,\n",
    "            min_instances=0,\n",
    "            max_instances=1,\n",
    "        )\n",
    "        ml_client.begin_create_or_update(compute_config).result()\n",
    "else:\n",
    "    gpu_compute_name = f\"{compute_version}-instance\"\n",
    "    try:\n",
    "        _ = ml_client.compute.get(gpu_compute_name)\n",
    "        print(\"Found existing compute instance.\")\n",
    "    except ResourceNotFoundError:\n",
    "        print(\"Creating a new compute instance...\")\n",
    "        compute_config = ComputeInstance(\n",
    "            name=gpu_compute_name,\n",
    "            size=compute_type\n",
    "        )\n",
    "        ml_client.begin_create_or_update(compute_config).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73644081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute cluster.\n"
     ]
    }
   ],
   "source": [
    "# Pipeline level compute\n",
    "compute_cluster = True\n",
    "compute_type = \"Standard_DS3_v2\" #\"Standard_NC12s_v3\"\n",
    "pipeline_level_compute_name = \"cpu-cluster\"\n",
    "\n",
    "if compute_cluster:\n",
    "    try:\n",
    "        _ = ml_client.compute.get(pipeline_level_compute_name)\n",
    "        print(\"Found existing compute cluster.\")\n",
    "    except ResourceNotFoundError:\n",
    "        print(\"Creating a new compute cluster...\")\n",
    "        compute_config = AmlCompute(\n",
    "            name=pipeline_level_compute_name,\n",
    "            type=\"amlcompute\",\n",
    "            size=compute_type,\n",
    "            idle_time_before_scale_down=120,\n",
    "            min_instances=0,\n",
    "            max_instances=1,\n",
    "        )\n",
    "        ml_client.begin_create_or_update(compute_config).result()\n",
    "else:\n",
    "    try:\n",
    "        _ = ml_client.compute.get(pipeline_level_compute_name)\n",
    "        print(\"Found existing compute instance.\")\n",
    "    except ResourceNotFoundError:\n",
    "        print(\"Creating a new compute instance...\")\n",
    "        compute_config = ComputeInstance(\n",
    "            name=pipeline_level_compute_name,\n",
    "            size=compute_type\n",
    "        )\n",
    "        ml_client.begin_create_or_update(compute_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c4b77",
   "metadata": {},
   "source": [
    "## 4. Create the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e12e097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Environment({'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'msft-raft-finetuning-env', 'description': 'Environment for SLM Fine-tuning', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': True, 'id': '/subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourceGroups/lianatests/providers/Microsoft.MachineLearningServices/workspaces/humpbackwhales-aml/environments/msft-raft-finetuning-env/versions/84', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/linapalk2/code/Users/linapalk/RAFT/raft_finetuning_pipeline', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f85ae2ee3b0>, 'serialize': <msrest.serialization.Serializer object at 0x7f85ae2ee290>, 'version': '84', 'latest_version': None, 'conda_file': None, 'image': None, 'build': <azure.ai.ml.entities._assets.environment.BuildContext object at 0x7f85ae2ee110>, 'inference_config': None, 'os_type': 'Linux', 'arm_type': 'environment_version', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': None})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_docker_image = Environment(\n",
    "    build=BuildContext(path=\"docker_image\"),\n",
    "    name=\"msft-raft-finetuning-env\",\n",
    "    description=\"Environment for SLM Fine-tuning\",\n",
    ")\n",
    "\"\"\"\n",
    "env_docker_image = Environment(\n",
    "    image=\"pytorch/pytorch:latest\",\n",
    "    conda_file=\"docker_image/conda.yml\",\n",
    "    name=\"pytorch-raft-finetuning-env\",\n",
    "    description=\"Environment for SLM Fine-tuning\",\n",
    ")\n",
    "\"\"\"\n",
    "ml_client.environments.create_or_update(env_docker_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33748e85-49b4-4b15-8e99-ca89cffd7c31",
   "metadata": {},
   "source": [
    "## 6. Download Fine-tuned Model\n",
    "Temporal workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c87b66c-69cf-4d15-b586-412a313193e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"phi3-mini-128K-instruct\"\n",
    "\n",
    "def is_folder_empty(folder_path):\n",
    "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "        if not os.listdir(folder_path):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"The path {folder_path} does not exist or is not a directory.\")\n",
    "        return False\n",
    "\n",
    "local_dir = f\"./models/{model_folder}\"\n",
    "if is_folder_empty(local_dir):\n",
    "    blob_service_client = BlobServiceClient(\n",
    "        account_url=os.getenv(\"STORAGE_ACCOUNT_URL\"),\n",
    "        credential=os.getenv(\"STORAGE_ACCOUNT_CREDENTIAL\")\n",
    "    )\n",
    "    container_name = \"azureml-blobstore-f4e80658-2a40-4426-8676-c5538afa6a8f\"\n",
    "    blob_prefix = \"azureml/814db0e1-86d7-40a2-bb82-fe9f565e461c/model_dir/\"\n",
    "\n",
    "    # Create the local directory if not exists\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    # List blobs and download them\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_list = container_client.list_blobs(name_starts_with=blob_prefix)\n",
    "\n",
    "    for blob in blob_list:\n",
    "        blob_client = blob_service_client.get_blob_client(container_name, blob.name)\n",
    "        local_file_path = os.path.join(local_dir, os.path.basename(blob.name))\n",
    "\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            download_stream = blob_client.download_blob()\n",
    "            f.write(download_stream.readall())\n",
    "            print(f\"Downloaded {blob.name} to {local_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c6781",
   "metadata": {},
   "source": [
    "## 7. Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab0f6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \".\"\n",
    "batch_inference_model_func = load_component(source=parent_dir + \"/score-model.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbc2846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline()\n",
    "def score():\n",
    "    inputs = {\n",
    "        \"base_model_id\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "        \"model_version\": \"phi3-mini-128K-instruct\",\n",
    "        \"ground_truth_file\": Input(\n",
    "            type=AssetTypes.URI_FILE, path=\"./data/data_final.csv\" #\"./data/raft_sample_data-ft.train.jsonl\"\n",
    "        ),\n",
    "        \"model_dir\": Input(\n",
    "            type=AssetTypes.URI_FOLDER, path=f\"./models/{model_folder}\"\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    outputs = {\n",
    "        \"inference_result\": Output(type=AssetTypes.URI_FOLDER)\n",
    "    }\n",
    "\n",
    "    score_model = batch_inference_model_func(\n",
    "        base_model_id=inputs[\"base_model_id\"],\n",
    "        model_version=inputs[\"model_version\"],\n",
    "        ground_truth_file=inputs[\"ground_truth_file\"],\n",
    "        model_dir=inputs[\"model_dir\"]\n",
    "    )\n",
    "    score_model.compute = gpu_compute_name\n",
    "    \n",
    "    return {\"inference_result\": score_model.outputs.inference_result}\n",
    "\n",
    "\n",
    "pipeline_job = score()\n",
    "\n",
    "pipeline_job.settings.default_compute = pipeline_level_compute_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02e6ec",
   "metadata": {},
   "source": [
    "## 7. Submit Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34aaadf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
      "\n",
      "Example: azcopy copy '/mnt/batch/tasks/shared/LS_root/mounts/clusters/linapalk2/code/Users/linapalk/RAFT/raft_finetuning_pipeline/models/phi3-mini-128K-instruct' 'https://humpbackwhalessa.blob.core.windows.net/azureml-blobstore-f4e80658-2a40-4426-8676-c5538afa6a8f/LocalUpload/d15237a7b92c47f383442de933ef858c/phi3-mini-128K-instruct' \n",
      "\n",
      "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
      "\u001b[32mUploading phi3-mini-128K-instruct (1617.09 MBs): 100%|██████████| 1617088822/1617088822 [00:03<00:00, 486820238.08it/s]\n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading src (0.04 MBs): 100%|██████████| 42229/42229 [00:00<00:00, 692453.37it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>raft-phi3-batch-evaluation</td><td>orange_dream_9x99g7yn9v</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/orange_dream_9x99g7yn9v?wsid=/subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourcegroups/lianatests/workspaces/humpbackwhales-aml&amp;tid=16b3c013-d300-468d-ac64-7eda0820b6d3\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {}, 'outputs': {'evaluation_result': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f85ae2ecdc0>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/linapalk2/code/Users/linapalk/RAFT/raft_finetuning_pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f858d323550>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'evaluate', 'is_deterministic': None, 'inputs': {}, 'outputs': {'evaluation_result': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'evaluate_model': Command({'parameters': {}, 'init': False, 'name': 'evaluate_model', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/linapalk2/code/Users/linapalk/RAFT/raft_finetuning_pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f858d323a00>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': 'nc24ads-a100-v4-cluster', 'services': None, 'comment': None, 'job_inputs': {'base_model_id': 'microsoft/Phi-3-mini-128k-instruct', 'model_version': 'phi3-mini-128K-instruct', 'ground_truth_file': {'type': 'uri_file', 'path': 'azureml://datastores/workspaceblobstore/paths/LocalUpload/68e3b9555d20815c3e5b44816a08f6c8/data_final.csv'}, 'model_dir': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceblobstore/paths/LocalUpload/d15237a7b92c47f383442de933ef858c/phi3-mini-128K-instruct/'}}, 'job_outputs': {'evaluation_result': '${{parent.outputs.evaluation_result}}'}, 'inputs': {'base_model_id': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f858d323670>, 'model_version': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f858d323c40>, 'ground_truth_file': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f858d323f10>, 'model_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f858d322b60>}, 'outputs': {'evaluation_result': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f858d323580>}, 'component': 'azureml_anonymous:a07eb48e-644c-4963-9369-36379e0c93fc', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'ed86b097-3c53-4b93-83c3-c6f0eee2c420', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': <azure.ai.ml.entities._job.distribution.PyTorchDistribution object at 0x7f858d323fd0>, 'environment_variables': {}, 'environment': None, 'resources': {'instance_count': 1}, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 1}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 1}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'orange_dream_9x99g7yn9v', 'description': None, 'tags': {}, 'properties': {'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'cpu-cluster', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourceGroups/lianatests/providers/Microsoft.MachineLearningServices/workspaces/humpbackwhales-aml/jobs/orange_dream_9x99g7yn9v', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/linapalk2/code/Users/linapalk/RAFT/raft_finetuning_pipeline', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f858d3223b0>, 'serialize': <msrest.serialization.Serializer object at 0x7f85b060faf0>, 'display_name': 'evaluate', 'experiment_name': 'raft-phi3-batch-evaluation', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://westeurope.api.azureml.ms/mlflow/v1.0/subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourceGroups/lianatests/providers/Microsoft.MachineLearningServices/workspaces/humpbackwhales-aml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/orange_dream_9x99g7yn9v?wsid=/subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourcegroups/lianatests/workspaces/humpbackwhales-aml&tid=16b3c013-d300-468d-ac64-7eda0820b6d3', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"raft-phi3-batch-inference\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c13b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: orange_dream_9x99g7yn9v\n",
      "Web View: https://ml.azure.com/runs/orange_dream_9x99g7yn9v?wsid=/subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourcegroups/lianatests/workspaces/humpbackwhales-aml\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-09-22 09:04:20Z] Submitting 1 runs, first five are: 6120d754:aa4da036-3e95-428f-aa9f-9c907c589e71\n"
     ]
    }
   ],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a249a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
