$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

name: finetune_model
display_name: Finetune Model

command: >
  python -m torch.distributed.run finetune_model.py
  --run-name ${{inputs.run_name}}
  --base-model-id ${{inputs.base_model_id}}
  --train-file ${{inputs.train_file}}
  --quantization-aware-training ${{inputs.quantization_aware_training}}
  --flash-attention ${{inputs.flash_attention}}
  --optimizer ${{inputs.optimizer}}
  --peft-approach ${{inputs.peft_approach}}
  --finetune-approach ${{inputs.finetune_approach}}
  --max-seq-length ${{inputs.max_seq_length}}
  --num-train-epochs ${{inputs.num_train_epochs}}
  --learning-rate ${{inputs.learning_rate}}
  --per-device-train-batch-size ${{inputs.per_device_train_batch_size}}
  --per-device-eval-batch-size ${{inputs.per_device_eval_batch_size}}
  --gradient-accumulation-steps ${{inputs.gradient_accumulation_steps}}
  --gradient-checkpointing ${{inputs.gradient_checkpointing}}
  --logging-steps ${{inputs.logging_steps}}
  --save-steps ${{inputs.save_steps}}
  --eval-steps ${{inputs.eval_steps}}
  --model-dir ${{outputs.model_dir}}

inputs:
  run_name:
    type: string
    description: Name of the run.
  base_model_id:
    type: string
    description: ID of the base model to fine-tune.
    default: "microsoft/Phi-3-mini-128k-instruct"
  train_file:
    type: uri_file
    description: URI of the training data file.
  quantization_aware_training:
    type: boolean
    description: Enable quantization-aware training.
    default: true
  flash_attention:
    type: boolean
    description: Enable flash attention.
    default: true
  optimizer:
    type: string
    description: Optimizer to use for training ('adamw_8bit' or 'adamw_torch_fused')
    default: "adamw_torch_fused"
    choices: ["adamw_8bit","adamw_torch_fused"]
  peft_approach:
    type: string
    description: Choose the PEFT approach ('qlora', 'dora' or 'lora').
    default: "lora"
    choices: ["qlora", "dora", "lora"]
  finetune_approach:
    type: string
    description: Choose the approach for fine-tuning a model ('sfttrainer' or 'unsloth').
    default: "sfttrainer"
    choices: ["sfttrainer", "unsloth"]
  max_seq_length:
    type: integer
    description: Maximum sequence length for the model.
    default: 4096
  num_train_epochs:
    type: integer
    description: Number of training epochs.
    default: 1
  learning_rate:
    type: number
    description: Learning rate for the optimizer.
    default: 1e-4
  per_device_train_batch_size:
    type: integer
    description: Batch size per device during training.
    default: 3
  per_device_eval_batch_size:
    type: integer
    description: Batch size per device during evaluation.
    default: 3
  gradient_accumulation_steps:
    type: integer
    description: Number of gradient accumulation steps.
    default: 6
  gradient_checkpointing:
    type: boolean
    description: Whether to use gradient checkpointing.
    default: true
  logging_steps:
    type: integer
    description: Number of steps between each logging.
    default: 10
  save_steps:
    type: integer
    description: Number of steps between each checkpoint save.
    default: 10
  eval_steps:
    type: integer
    description: Number of steps between each evaluation.
    default: 10

# Define the outputs
outputs:
  model_dir: 
    type: uri_folder

# Code location
code: ./finetuning

# Distribution settings for PyTorch
distribution:
  type: pytorch
  process_count_per_instance: 1

# Resource configuration
resources:
  instance_count: 1

# Reference to the pre-configured environment
environment: azureml:gpu-finetuning-environment@latest