{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## This notebook is to evaluate SLM Vs LLM Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1726827805862
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38/lib/python3.9/site-packages/deepeval/__init__.py:49: UserWarning: You are using deepeval version 1.2.1, however version 1.2.2 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import json\n",
        "import ssl\n",
        "from deepeval.metrics import AnswerRelevancyMetric, ContextualRelevancyMetric, ContextualPrecisionMetric, ContextualRecallMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "class AzureOpenAIEvaluator(DeepEvalBaseLLM):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model\n",
        "    ):\n",
        "        self.model = model\n",
        "\n",
        "    def load_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        chat_model = self.load_model()\n",
        "        return chat_model.invoke(prompt).content\n",
        "\n",
        "    async def a_generate(self, prompt: str) -> str:\n",
        "        chat_model = self.load_model()\n",
        "        res = await chat_model.ainvoke(prompt)\n",
        "        return res.content\n",
        "\n",
        "    def get_model_name(self):\n",
        "        return \"Custom Azure OpenAI Model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1726827827428
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import json\n",
        "import ssl\n",
        "from deepeval.metrics import AnswerRelevancyMetric, ContextualRelevancyMetric, ContextualPrecisionMetric, ContextualRecallMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "nltk.download('punkt')\n",
        "\n",
        "class RAG_Evaluator():\n",
        "    def __init__(self, framework):\n",
        "        self.framework = framework\n",
        "        if self.framework == \"deepeval\":\n",
        "            custom_model = AzureChatOpenAI(\n",
        "                openai_api_version=\"2023-03-15-preview\",\n",
        "                azure_deployment=\"gpt4omini\",\n",
        "                azure_endpoint=\"\",\n",
        "                openai_api_key=\"\",\n",
        "            )\n",
        "            azure_openai = AzureOpenAIEvaluator(model=custom_model)\n",
        "            self.answer_relevancy_metric = AnswerRelevancyMetric(\n",
        "                model=azure_openai,\n",
        "                threshold=0.7,\n",
        "                include_reason=True\n",
        "            )\n",
        "            self.context_relevancy_metric = ContextualRelevancyMetric(\n",
        "                model=azure_openai,\n",
        "                threshold=0.7,\n",
        "                include_reason=True\n",
        "            )\n",
        "            self.context_precision_metric = ContextualPrecisionMetric(\n",
        "                threshold=0.7,\n",
        "                model=azure_openai,\n",
        "                include_reason=True\n",
        "            )\n",
        "            self.context_recall_metric = ContextualRecallMetric(\n",
        "                threshold=0.7,\n",
        "                model=azure_openai,\n",
        "                include_reason=True\n",
        "            )\n",
        "\n",
        "    def check_answer_relevancy(self, question, actual_output):\n",
        "        score = \"\"\n",
        "        reason = \"\"\n",
        "\n",
        "        if self.framework == \"deepeval\":\n",
        "            test_case = LLMTestCase(\n",
        "                input=question,\n",
        "                actual_output=actual_output\n",
        "            )\n",
        "            self.answer_relevancy_metric.measure(test_case)\n",
        "            score = self.answer_relevancy_metric.score\n",
        "            reason = self.answer_relevancy_metric.reason\n",
        "\n",
        "        return score, reason\n",
        "\n",
        "    def check_contextual_relevancy(self, question, actual_output, retrieval_contexts):\n",
        "        score = \"\"\n",
        "        reason = \"\"\n",
        "\n",
        "        if self.framework == \"deepeval\":\n",
        "            test_case = LLMTestCase(\n",
        "                input=question,\n",
        "                actual_output=actual_output,\n",
        "                retrieval_context=retrieval_contexts\n",
        "            )\n",
        "            self.context_relevancy_metric.measure(test_case)\n",
        "            score = self.context_relevancy_metric.score\n",
        "            reason = self.context_relevancy_metric.reason\n",
        "\n",
        "        return score, reason\n",
        "\n",
        "    def check_contextual_precision(self, question, actual_output, expected_output, retrieval_contexts):\n",
        "        score = \"\"\n",
        "        reason = \"\"\n",
        "\n",
        "        if self.framework == \"deepeval\":\n",
        "            test_case = LLMTestCase(\n",
        "                input=question,\n",
        "                actual_output=actual_output,\n",
        "                expected_output=expected_output,\n",
        "                retrieval_context=retrieval_contexts\n",
        "            )\n",
        "            self.context_precision_metric.measure(test_case)\n",
        "            score = self.context_precision_metric.score\n",
        "            reason = self.context_precision_metric.reason\n",
        "\n",
        "        return score, reason\n",
        "\n",
        "    def check_contextual_recall(self, question, actual_output, expected_output, retrieval_contexts):\n",
        "        score = \"\"\n",
        "        reason = \"\"\n",
        "\n",
        "        if self.framework == \"deepeval\":\n",
        "            test_case = LLMTestCase(\n",
        "                input=question,\n",
        "                actual_output=actual_output,\n",
        "                expected_output=expected_output,\n",
        "                retrieval_context=retrieval_contexts\n",
        "            )\n",
        "            self.context_recall_metric.measure(test_case)\n",
        "            score = self.context_recall_metric.score\n",
        "            reason = self.context_recall_metric.reason\n",
        "\n",
        "        return score, reason\n",
        "\n",
        "    def calculate_metrics(self, ground_truth, llm_response):\n",
        "        def calculate_rouge(reference, hypothesis):\n",
        "            scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "            scores = scorer.score(reference, hypothesis)\n",
        "            return scores\n",
        "\n",
        "        def calculate_bertscore(reference, hypothesis):\n",
        "            P, R, F1 = score([hypothesis], [reference], lang=\"en\", verbose=False)\n",
        "            return {\n",
        "                'precision': P.item(),\n",
        "                'recall': R.item(),\n",
        "                'f1': F1.item()\n",
        "            }\n",
        "\n",
        "        rouge_scores = calculate_rouge(ground_truth, llm_response)\n",
        "        bertscore = calculate_bertscore(ground_truth, llm_response)\n",
        "\n",
        "        results = {\n",
        "            'ROUGE-1': rouge_scores['rouge1'].fmeasure,\n",
        "            'ROUGE-2': rouge_scores['rouge2'].fmeasure,\n",
        "            'ROUGE-L': rouge_scores['rougeL'].fmeasure,\n",
        "            'BERTScore Precision': bertscore['precision'],\n",
        "            'BERTScore Recall': bertscore['recall'],\n",
        "            'BERTScore F1': bertscore['f1']\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def perform_ragas_evaluation(self, question, expected_response, rag_response):\n",
        "        rag_response_str = rag_response[\"response_output\"]\n",
        "        metrics = {}\n",
        "        \n",
        "        answer_relevancy_score, answer_relevancy_reason = self.check_answer_relevancy(question, rag_response_str)\n",
        "        # time.sleep(5)\n",
        "\n",
        "        relevant_contexts = [source_doc for source_doc in rag_response[\"source_documents\"]]\n",
        "        contextual_relevancy_score, contextual_relevancy_reason = self.check_contextual_relevancy(question, rag_response_str, relevant_contexts)\n",
        "        # time.sleep(5)\n",
        "\n",
        "        if expected_response != \"\":\n",
        "            contextual_precision_score, contextual_precision_reason = self.check_contextual_precision(question, rag_response_str, expected_response, relevant_contexts)\n",
        "            # time.sleep(5)\n",
        "            contextual_recall_score, contextual_recall_reason = self.check_contextual_recall(question, rag_response_str, expected_response, relevant_contexts)\n",
        "            # time.sleep(5)\n",
        "        else:\n",
        "            contextual_precision_score, contextual_precision_reason = 0.0, \"NA\"\n",
        "            contextual_recall_score, contextual_recall_reason = 0.0, \"NA\"\n",
        "        \n",
        "        metrics.update({\n",
        "            'answer_relevancy_score': answer_relevancy_score,\n",
        "            'answer_relevancy_reason': answer_relevancy_reason,\n",
        "            'contextual_relevancy_score': contextual_relevancy_score,\n",
        "            'contextual_relevancy_reason': contextual_relevancy_reason,\n",
        "            'contextual_precision_score': contextual_precision_score,\n",
        "            'contextual_precision_reason': contextual_precision_reason,\n",
        "            'contextual_recall_score': contextual_recall_score,\n",
        "            'contextual_recall_reason': contextual_recall_reason\n",
        "        })\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def perform_NLP_evaluation(self, question, expected_response, rag_response):\n",
        "        rag_response_str = rag_response[\"response_output\"]\n",
        "        additional_metrics = {}     \n",
        "        additional_metrics = self.calculate_metrics(expected_response, rag_response_str)\n",
        "        return additional_metrics\n",
        "    \n",
        "    def get_rag_response(self, question):\n",
        "        starttime = time.time()\n",
        "        response = \"\"\n",
        "        final_dict = {}\n",
        "        def allowSelfSignedHttps(allowed):\n",
        "                \"\"\"Bypass the server certificate verification on the client side.\"\"\"\n",
        "                if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
        "                    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "        allowSelfSignedHttps(True)  # This is needed if you use self-signed certificates.\n",
        "\n",
        "        def send_chat_input_LLM(question):\n",
        "            \"\"\"Send a chat input to the API and return the parsed response.\"\"\"\n",
        "            # API request payload\n",
        "            data = {\n",
        "                \"chat_input\": question,\n",
        "                \"chat_history\": []  # Modify this if you have previous chat history\n",
        "            }\n",
        "            \n",
        "            # API credentials\n",
        "            api_key = \"\"  # Replace with your actual API key\n",
        "            url = \"\"  # Replace with the actual API URL\n",
        "\n",
        "            # Prepare request body and headers\n",
        "            body = str.encode(json.dumps(data))\n",
        "            \n",
        "            if not api_key:\n",
        "                raise Exception(\"A valid API key is required to invoke the endpoint.\")\n",
        "            \n",
        "            headers = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + api_key}\n",
        "            req = urllib.request.Request(url, body, headers)\n",
        "            \n",
        "            # Initialize response structure\n",
        "            text_docs = []\n",
        "            final_dict = {}\n",
        "\n",
        "            try:\n",
        "                # Make the API call\n",
        "                response = urllib.request.urlopen(req)\n",
        "                result = response.read().decode('utf-8')\n",
        "                \n",
        "                # Parse the JSON response\n",
        "                result_dict = json.loads(result)\n",
        "                chat_output = result_dict.get('chat_output', {})\n",
        "                \n",
        "                # Extract chat response data\n",
        "                response_output = chat_output.get('response', 'No response found')\n",
        "                user_question = chat_output.get('user_question', 'No question found')\n",
        "                source_documents = chat_output.get('source_documents', [])\n",
        "                \n",
        "                # Collect source document texts\n",
        "                for doc in source_documents:\n",
        "                    text = doc.get('text', 'No text found')\n",
        "                    text_docs.append(text)\n",
        "                \n",
        "                # Build the response dictionary\n",
        "                final_dict['response_output'] = response_output\n",
        "                final_dict['user_question'] = user_question\n",
        "                final_dict['source_documents'] = text_docs\n",
        "                endtime = time.time()\n",
        "                final_dict['infernce_time_llm'] = endtime-starttime\n",
        "                return final_dict\n",
        "\n",
        "            except urllib.error.HTTPError as error:\n",
        "                print(f\"The request failed with status code: {error.code}\")\n",
        "                print(error.info())\n",
        "                print(error.read().decode(\"utf8\", 'ignore'))\n",
        "                return None          \n",
        "            \n",
        "            return response_dict\n",
        "        response_dictionary = send_chat_input_LLM(question)\n",
        "        return response_dictionary\n",
        "def evaluate_responses(df, evaluator,evaluation_type = 'RAGAS', model_type='LLM'):\n",
        "    \"\"\"\n",
        "    Evaluates LLM or SLM responses and Deepeval scores for each row in the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): DataFrame containing columns 'question' and 'answer'.\n",
        "    evaluator (RAG_Evaluator): An instance of the RAG_Evaluator with a specified framework.\n",
        "    model_type (str): Specifies whether to use 'LLM' or 'SLM' for the response evaluation.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: DataFrame with added columns for model responses and Deepeval scores.\n",
        "    \"\"\"\n",
        "    # Determine the prefix based on the model type\n",
        "    prefix = model_type.upper()\n",
        "\n",
        "    # # Separate the response output and source documents into different columns\n",
        "    # df[f'response_output_{model_type}'] = df[f'{prefix}_response'].apply(lambda x: x['response_output'])\n",
        "    # df[f'source_documents_{model_type}'] = df[f'{prefix}_response'].apply(lambda x: x['source_documents'])\n",
        "\n",
        "    if evaluation_type == 'RAGAS':\n",
        "    # Apply the evaluation model to score the responses\n",
        "        df[f'deepeval_response_{model_type}'] = df.apply(\n",
        "            lambda row: evaluator.perform_ragas_evaluation(row['question'], row['answer'], row[f'{prefix}_response']), axis=1)\n",
        "\n",
        "        # Separate the evaluation scores into different columns\n",
        "        df[f'answer_relevancy_score_{model_type}'] = df[f'deepeval_response_{model_type}'].apply(\n",
        "            lambda x: x['answer_relevancy_score'])\n",
        "        df[f'contextual_relevancy_score_{model_type}'] = df[f'deepeval_response_{model_type}'].apply(\n",
        "            lambda x: x['contextual_relevancy_score'])\n",
        "        df[f'contextual_precision_score_{model_type}'] = df[f'deepeval_response_{model_type}'].apply(\n",
        "            lambda x: x['contextual_precision_score'])\n",
        "        df[f'contextual_recall_score_{model_type}'] = df[f'deepeval_response_{model_type}'].apply(\n",
        "            lambda x: x['contextual_recall_score'])\n",
        "    elif evaluation_type == 'NLP':\n",
        "        df[f'deepeval_NLP_response_{model_type}'] = df.apply(\n",
        "            lambda row: evaluator.perform_NLP_evaluation(row['question'], row['answer'], row[f'{prefix}_response']), axis=1)\n",
        "\n",
        "        # Append additional evaluation metrics\n",
        "        df[f'ROUGE-1_{model_type}'] = df[f'deepeval_NLP_response_{model_type}'].apply(lambda x: x['ROUGE-1'])\n",
        "        df[f'ROUGE-2_{model_type}'] = df[f'deepeval_NLP_response_{model_type}'].apply(lambda x: x['ROUGE-2'])\n",
        "        df[f'ROUGE-L_{model_type}'] = df[f'deepeval_NLP_response_{model_type}'].apply(lambda x: x['ROUGE-L'])\n",
        "        df[f'BERTScore_Precision_{model_type}'] = df[f'deepeval_NLP_response_{model_type}'].apply(lambda x: x['BERTScore Precision'])\n",
        "        df[f'BERTScore_Recall_{model_type}'] = df[f'deepeval_NLP_response_{model_type}'].apply(lambda x: x['BERTScore Recall'])\n",
        "        df[f'BERTScore_F1_{model_type}'] = df[f'deepeval_NLP_response_{model_type}'].apply(lambda x: x['BERTScore F1'])\n",
        "\n",
        "    # Optional: Drop the intermediate deepeval response column if not needed\n",
        "    # df.drop(columns=[f'deepeval_response_{model_type}'], inplace=True)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1726827832812
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>chapter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the main purpose of the microplate rea...</td>\n",
              "      <td>The microplate reader is used to read the resu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What type of test is a microplate reader prima...</td>\n",
              "      <td>It is primarily used for the ELISA (Enzyme-Lin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Describe the wavelength range typically used b...</td>\n",
              "      <td>Microplate readers typically operate within a ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What are the key components required for an EL...</td>\n",
              "      <td>Key components include a microplate reader, mi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the different phases involved in an E...</td>\n",
              "      <td>ELISA involves coating wells with antibodies/a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What is the main purpose of the microplate rea...   \n",
              "1  What type of test is a microplate reader prima...   \n",
              "2  Describe the wavelength range typically used b...   \n",
              "3  What are the key components required for an EL...   \n",
              "4  What are the different phases involved in an E...   \n",
              "\n",
              "                                              answer  chapter  \n",
              "0  The microplate reader is used to read the resu...        1  \n",
              "1  It is primarily used for the ELISA (Enzyme-Lin...        1  \n",
              "2  Microplate readers typically operate within a ...        1  \n",
              "3  Key components include a microplate reader, mi...        1  \n",
              "4  ELISA involves coating wells with antibodies/a...        1  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read ground truth\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "import nest_asyncio\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Event loop is already running.*\")\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Path to your JSONL file\n",
        "file_path = \"lab_maintenance_100_qa.jsonl\"\n",
        "\n",
        "# Read the JSON file as a list of dictionaries\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)  # Use json.load() since it's a JSON array, not a JSONL file\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "df.head()\n",
        "\n",
        "# df=df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1726827836593
        }
      },
      "outputs": [],
      "source": [
        "evaluator = RAG_Evaluator(framework = \"deepeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1726828075225
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# call Prompt flow to generate response \n",
        "# Apply the RAG model to generate responses for each question\n",
        "prefix = 'LLM'\n",
        "df[f'{prefix}_response'] = df.apply(lambda row: evaluator.get_rag_response(row['question']), axis=1)\n",
        "    # Separate the response output and source documents into different columns\n",
        "df[f'response_output_{prefix}'] = df[f'{prefix}_response'].apply(lambda x: x['response_output'])\n",
        "df[f'context'] = df[f'{prefix}_response'].apply(lambda x: x['source_documents'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1726828132479
        }
      },
      "outputs": [],
      "source": [
        "df[f'LLM_Responsetime'] = df[f'{prefix}_response'].apply(lambda x: x['infernce_time_llm'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1726828139933
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>chapter</th>\n",
              "      <th>LLM_response</th>\n",
              "      <th>response_output_LLM</th>\n",
              "      <th>context</th>\n",
              "      <th>LLM_Responsetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the main purpose of the microplate rea...</td>\n",
              "      <td>The microplate reader is used to read the resu...</td>\n",
              "      <td>1</td>\n",
              "      <td>{'response_output': 'The main purpose of the m...</td>\n",
              "      <td>The main purpose of the microplate reader is t...</td>\n",
              "      <td>[Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...</td>\n",
              "      <td>2.023033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What type of test is a microplate reader prima...</td>\n",
              "      <td>It is primarily used for the ELISA (Enzyme-Lin...</td>\n",
              "      <td>1</td>\n",
              "      <td>{'response_output': 'A microplate reader is pr...</td>\n",
              "      <td>A microplate reader is primarily used for read...</td>\n",
              "      <td>[Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...</td>\n",
              "      <td>1.482003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Describe the wavelength range typically used b...</td>\n",
              "      <td>Microplate readers typically operate within a ...</td>\n",
              "      <td>1</td>\n",
              "      <td>{'response_output': 'The wavelength range typi...</td>\n",
              "      <td>The wavelength range typically used by a micro...</td>\n",
              "      <td>[Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...</td>\n",
              "      <td>1.365792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What are the key components required for an EL...</td>\n",
              "      <td>Key components include a microplate reader, mi...</td>\n",
              "      <td>1</td>\n",
              "      <td>{'response_output': 'The key components requir...</td>\n",
              "      <td>The key components required for an ELISA test ...</td>\n",
              "      <td>[Title: data.pdfTABLE OF FIGURES\\nviiiTable of...</td>\n",
              "      <td>1.322968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the different phases involved in an E...</td>\n",
              "      <td>ELISA involves coating wells with antibodies/a...</td>\n",
              "      <td>1</td>\n",
              "      <td>{'response_output': 'The ELISA technique invol...</td>\n",
              "      <td>The ELISA technique involves the following pha...</td>\n",
              "      <td>[Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...</td>\n",
              "      <td>3.635758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What is the main purpose of the microplate rea...   \n",
              "1  What type of test is a microplate reader prima...   \n",
              "2  Describe the wavelength range typically used b...   \n",
              "3  What are the key components required for an EL...   \n",
              "4  What are the different phases involved in an E...   \n",
              "\n",
              "                                              answer  chapter  \\\n",
              "0  The microplate reader is used to read the resu...        1   \n",
              "1  It is primarily used for the ELISA (Enzyme-Lin...        1   \n",
              "2  Microplate readers typically operate within a ...        1   \n",
              "3  Key components include a microplate reader, mi...        1   \n",
              "4  ELISA involves coating wells with antibodies/a...        1   \n",
              "\n",
              "                                        LLM_response  \\\n",
              "0  {'response_output': 'The main purpose of the m...   \n",
              "1  {'response_output': 'A microplate reader is pr...   \n",
              "2  {'response_output': 'The wavelength range typi...   \n",
              "3  {'response_output': 'The key components requir...   \n",
              "4  {'response_output': 'The ELISA technique invol...   \n",
              "\n",
              "                                 response_output_LLM  \\\n",
              "0  The main purpose of the microplate reader is t...   \n",
              "1  A microplate reader is primarily used for read...   \n",
              "2  The wavelength range typically used by a micro...   \n",
              "3  The key components required for an ELISA test ...   \n",
              "4  The ELISA technique involves the following pha...   \n",
              "\n",
              "                                             context  LLM_Responsetime  \n",
              "0  [Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...          2.023033  \n",
              "1  [Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...          1.482003  \n",
              "2  [Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...          1.365792  \n",
              "3  [Title: data.pdfTABLE OF FIGURES\\nviiiTable of...          1.322968  \n",
              "4  [Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...          3.635758  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1726828155456
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"data_time_response.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# get RAGAS response for LLM\n",
        "df_eval = evaluate_responses(df,evaluator,'RAGAS','LLM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1726760765547
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_eval' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_eval\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_n.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_eval' is not defined"
          ]
        }
      ],
      "source": [
        "df_eval.to_csv(\"data_time_response.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "# get NLP response for LLM\n",
        "final_output = evaluate_responses(df_eval,evaluator,'NLP','LLM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>chapter</th>\n",
              "      <th>LLM_response</th>\n",
              "      <th>response_output_LLM</th>\n",
              "      <th>context</th>\n",
              "      <th>deepeval_response_LLM</th>\n",
              "      <th>answer_relevancy_score_LLM</th>\n",
              "      <th>contextual_relevancy_score_LLM</th>\n",
              "      <th>contextual_precision_score_LLM</th>\n",
              "      <th>contextual_recall_score_LLM</th>\n",
              "      <th>deepeval_NLP_response_LLM</th>\n",
              "      <th>ROUGE-1_LLM</th>\n",
              "      <th>ROUGE-2_LLM</th>\n",
              "      <th>ROUGE-L_LLM</th>\n",
              "      <th>BERTScore_Precision_LLM</th>\n",
              "      <th>BERTScore_Recall_LLM</th>\n",
              "      <th>BERTScore_F1_LLM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the main purpose of the microplate rea...</td>\n",
              "      <td>The microplate reader is used to read the resu...</td>\n",
              "      <td>1</td>\n",
              "      <td>{'response_output': 'The main purpose of the m...</td>\n",
              "      <td>The main purpose of the microplate reader is t...</td>\n",
              "      <td>[Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...</td>\n",
              "      <td>{'answer_relevancy_score': 0.6666666666666666,...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>{'ROUGE-1': 0.6909090909090908, 'ROUGE-2': 0.5...</td>\n",
              "      <td>0.690909</td>\n",
              "      <td>0.528302</td>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.911905</td>\n",
              "      <td>0.947884</td>\n",
              "      <td>0.929547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What type of test is a microplate reader prima...</td>\n",
              "      <td>It is primarily used for the ELISA (Enzyme-Lin...</td>\n",
              "      <td>1</td>\n",
              "      <td>{'response_output': 'A microplate reader is pr...</td>\n",
              "      <td>A microplate reader is primarily used for read...</td>\n",
              "      <td>[Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...</td>\n",
              "      <td>{'answer_relevancy_score': 0.6666666666666666,...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>{'ROUGE-1': 0.5098039215686274, 'ROUGE-2': 0.1...</td>\n",
              "      <td>0.509804</td>\n",
              "      <td>0.163265</td>\n",
              "      <td>0.431373</td>\n",
              "      <td>0.894282</td>\n",
              "      <td>0.897778</td>\n",
              "      <td>0.896027</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What is the main purpose of the microplate rea...   \n",
              "1  What type of test is a microplate reader prima...   \n",
              "\n",
              "                                              answer  chapter  \\\n",
              "0  The microplate reader is used to read the resu...        1   \n",
              "1  It is primarily used for the ELISA (Enzyme-Lin...        1   \n",
              "\n",
              "                                        LLM_response  \\\n",
              "0  {'response_output': 'The main purpose of the m...   \n",
              "1  {'response_output': 'A microplate reader is pr...   \n",
              "\n",
              "                                 response_output_LLM  \\\n",
              "0  The main purpose of the microplate reader is t...   \n",
              "1  A microplate reader is primarily used for read...   \n",
              "\n",
              "                                             context  \\\n",
              "0  [Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...   \n",
              "1  [Title: data.pdfMAINTENANCE MANUAL FOR LABORAT...   \n",
              "\n",
              "                               deepeval_response_LLM  \\\n",
              "0  {'answer_relevancy_score': 0.6666666666666666,...   \n",
              "1  {'answer_relevancy_score': 0.6666666666666666,...   \n",
              "\n",
              "   answer_relevancy_score_LLM  contextual_relevancy_score_LLM  \\\n",
              "0                    0.666667                        0.666667   \n",
              "1                    0.666667                        1.000000   \n",
              "\n",
              "   contextual_precision_score_LLM  contextual_recall_score_LLM  \\\n",
              "0                             1.0                          1.0   \n",
              "1                             1.0                          1.0   \n",
              "\n",
              "                           deepeval_NLP_response_LLM  ROUGE-1_LLM  \\\n",
              "0  {'ROUGE-1': 0.6909090909090908, 'ROUGE-2': 0.5...     0.690909   \n",
              "1  {'ROUGE-1': 0.5098039215686274, 'ROUGE-2': 0.1...     0.509804   \n",
              "\n",
              "   ROUGE-2_LLM  ROUGE-L_LLM  BERTScore_Precision_LLM  BERTScore_Recall_LLM  \\\n",
              "0     0.528302     0.618182                 0.911905              0.947884   \n",
              "1     0.163265     0.431373                 0.894282              0.897778   \n",
              "\n",
              "   BERTScore_F1_LLM  \n",
              "0          0.929547  \n",
              "1          0.896027  "
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {
          "2b6cf70d553f44939f3c6ae1855d37b0": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "1.5.0",
            "model_name": "FloatProgressModel",
            "state": {
              "_dom_classes": [],
              "_model_module": "@jupyter-widgets/controls",
              "_model_module_version": "1.5.0",
              "_model_name": "FloatProgressModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/controls",
              "_view_module_version": "1.5.0",
              "_view_name": "ProgressView",
              "bar_style": "success",
              "description": "",
              "description_allow_html": false,
              "description_tooltip": null,
              "layout": "IPY_MODEL_2d778339b4ef4007904b2022aceb1434",
              "max": 1,
              "min": 0,
              "orientation": "horizontal",
              "style": "IPY_MODEL_80c6c4377d834442b1477fc9754f1b55",
              "tabbable": null,
              "tooltip": null,
              "value": 1
            }
          },
          "2d778339b4ef4007904b2022aceb1434": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "1.2.0",
            "model_name": "LayoutModel",
            "state": {
              "_model_module": "@jupyter-widgets/base",
              "_model_module_version": "1.2.0",
              "_model_name": "LayoutModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/base",
              "_view_module_version": "1.2.0",
              "_view_name": "LayoutView",
              "align_content": null,
              "align_items": null,
              "align_self": null,
              "border": null,
              "border_bottom": null,
              "border_left": null,
              "border_right": null,
              "border_top": null,
              "bottom": null,
              "display": null,
              "flex": null,
              "flex_flow": null,
              "grid_area": null,
              "grid_auto_columns": null,
              "grid_auto_flow": null,
              "grid_auto_rows": null,
              "grid_column": null,
              "grid_gap": null,
              "grid_row": null,
              "grid_template_areas": null,
              "grid_template_columns": null,
              "grid_template_rows": null,
              "height": null,
              "justify_content": null,
              "justify_items": null,
              "left": null,
              "margin": null,
              "max_height": null,
              "max_width": null,
              "min_height": null,
              "min_width": null,
              "object_fit": null,
              "object_position": null,
              "order": null,
              "overflow": null,
              "overflow_x": null,
              "overflow_y": null,
              "padding": null,
              "right": null,
              "top": null,
              "visibility": null,
              "width": "20px"
            }
          },
          "4579b400de2942caa4a6d3fe0801f1f0": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "1.5.0",
            "model_name": "DescriptionStyleModel",
            "state": {
              "_model_module": "@jupyter-widgets/controls",
              "_model_module_version": "1.5.0",
              "_model_name": "DescriptionStyleModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/base",
              "_view_module_version": "1.2.0",
              "_view_name": "StyleView",
              "description_width": ""
            }
          },
          "4d1e1212643b4ad0b2ae9ef65e42f6f5": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "1.5.0",
            "model_name": "HTMLModel",
            "state": {
              "_dom_classes": [],
              "_model_module": "@jupyter-widgets/controls",
              "_model_module_version": "1.5.0",
              "_model_name": "HTMLModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/controls",
              "_view_module_version": "1.5.0",
              "_view_name": "HTMLView",
              "description": "",
              "description_allow_html": false,
              "description_tooltip": null,
              "disabled": false,
              "layout": "IPY_MODEL_7e55dc1023ed4117b7cfb0aacb5169e8",
              "placeholder": "​",
              "style": "IPY_MODEL_4579b400de2942caa4a6d3fe0801f1f0",
              "tabbable": null,
              "tooltip": null,
              "value": " 85/0 [00:00&lt;00:00, 547.27 examples/s]"
            }
          },
          "7e55dc1023ed4117b7cfb0aacb5169e8": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "1.2.0",
            "model_name": "LayoutModel",
            "state": {
              "_model_module": "@jupyter-widgets/base",
              "_model_module_version": "1.2.0",
              "_model_name": "LayoutModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/base",
              "_view_module_version": "1.2.0",
              "_view_name": "LayoutView",
              "align_content": null,
              "align_items": null,
              "align_self": null,
              "border": null,
              "border_bottom": null,
              "border_left": null,
              "border_right": null,
              "border_top": null,
              "bottom": null,
              "display": null,
              "flex": null,
              "flex_flow": null,
              "grid_area": null,
              "grid_auto_columns": null,
              "grid_auto_flow": null,
              "grid_auto_rows": null,
              "grid_column": null,
              "grid_gap": null,
              "grid_row": null,
              "grid_template_areas": null,
              "grid_template_columns": null,
              "grid_template_rows": null,
              "height": null,
              "justify_content": null,
              "justify_items": null,
              "left": null,
              "margin": null,
              "max_height": null,
              "max_width": null,
              "min_height": null,
              "min_width": null,
              "object_fit": null,
              "object_position": null,
              "order": null,
              "overflow": null,
              "overflow_x": null,
              "overflow_y": null,
              "padding": null,
              "right": null,
              "top": null,
              "visibility": null,
              "width": null
            }
          },
          "80c6c4377d834442b1477fc9754f1b55": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "1.5.0",
            "model_name": "ProgressStyleModel",
            "state": {
              "_model_module": "@jupyter-widgets/controls",
              "_model_module_version": "1.5.0",
              "_model_name": "ProgressStyleModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/base",
              "_view_module_version": "1.2.0",
              "_view_name": "StyleView",
              "bar_color": null,
              "description_width": ""
            }
          },
          "b9b91b22f462418c92e7d11a9090a0d0": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "1.5.0",
            "model_name": "HTMLModel",
            "state": {
              "_dom_classes": [],
              "_model_module": "@jupyter-widgets/controls",
              "_model_module_version": "1.5.0",
              "_model_name": "HTMLModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/controls",
              "_view_module_version": "1.5.0",
              "_view_name": "HTMLView",
              "description": "",
              "description_allow_html": false,
              "description_tooltip": null,
              "disabled": false,
              "layout": "IPY_MODEL_fb03a5b285d343e3a97afabcba3a1ff7",
              "placeholder": "​",
              "style": "IPY_MODEL_dc28b7e3ffdf44aaa902b30dbc364039",
              "tabbable": null,
              "tooltip": null,
              "value": "Generating train split: "
            }
          },
          "dc28b7e3ffdf44aaa902b30dbc364039": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "1.5.0",
            "model_name": "DescriptionStyleModel",
            "state": {
              "_model_module": "@jupyter-widgets/controls",
              "_model_module_version": "1.5.0",
              "_model_name": "DescriptionStyleModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/base",
              "_view_module_version": "1.2.0",
              "_view_name": "StyleView",
              "description_width": ""
            }
          },
          "ee20ef7cadc14ecaa8d77c39feff6415": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "1.2.0",
            "model_name": "LayoutModel",
            "state": {
              "_model_module": "@jupyter-widgets/base",
              "_model_module_version": "1.2.0",
              "_model_name": "LayoutModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/base",
              "_view_module_version": "1.2.0",
              "_view_name": "LayoutView",
              "align_content": null,
              "align_items": null,
              "align_self": null,
              "border": null,
              "border_bottom": null,
              "border_left": null,
              "border_right": null,
              "border_top": null,
              "bottom": null,
              "display": null,
              "flex": null,
              "flex_flow": null,
              "grid_area": null,
              "grid_auto_columns": null,
              "grid_auto_flow": null,
              "grid_auto_rows": null,
              "grid_column": null,
              "grid_gap": null,
              "grid_row": null,
              "grid_template_areas": null,
              "grid_template_columns": null,
              "grid_template_rows": null,
              "height": null,
              "justify_content": null,
              "justify_items": null,
              "left": null,
              "margin": null,
              "max_height": null,
              "max_width": null,
              "min_height": null,
              "min_width": null,
              "object_fit": null,
              "object_position": null,
              "order": null,
              "overflow": null,
              "overflow_x": null,
              "overflow_y": null,
              "padding": null,
              "right": null,
              "top": null,
              "visibility": null,
              "width": null
            }
          },
          "fb03a5b285d343e3a97afabcba3a1ff7": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "1.2.0",
            "model_name": "LayoutModel",
            "state": {
              "_model_module": "@jupyter-widgets/base",
              "_model_module_version": "1.2.0",
              "_model_name": "LayoutModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/base",
              "_view_module_version": "1.2.0",
              "_view_name": "LayoutView",
              "align_content": null,
              "align_items": null,
              "align_self": null,
              "border": null,
              "border_bottom": null,
              "border_left": null,
              "border_right": null,
              "border_top": null,
              "bottom": null,
              "display": null,
              "flex": null,
              "flex_flow": null,
              "grid_area": null,
              "grid_auto_columns": null,
              "grid_auto_flow": null,
              "grid_auto_rows": null,
              "grid_column": null,
              "grid_gap": null,
              "grid_row": null,
              "grid_template_areas": null,
              "grid_template_columns": null,
              "grid_template_rows": null,
              "height": null,
              "justify_content": null,
              "justify_items": null,
              "left": null,
              "margin": null,
              "max_height": null,
              "max_width": null,
              "min_height": null,
              "min_width": null,
              "object_fit": null,
              "object_position": null,
              "order": null,
              "overflow": null,
              "overflow_x": null,
              "overflow_y": null,
              "padding": null,
              "right": null,
              "top": null,
              "visibility": null,
              "width": null
            }
          },
          "ff3a32f2578a4ed2a631393882858828": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "1.5.0",
            "model_name": "HBoxModel",
            "state": {
              "_dom_classes": [],
              "_model_module": "@jupyter-widgets/controls",
              "_model_module_version": "1.5.0",
              "_model_name": "HBoxModel",
              "_view_count": null,
              "_view_module": "@jupyter-widgets/controls",
              "_view_module_version": "1.5.0",
              "_view_name": "HBoxView",
              "box_style": "",
              "children": [
                "IPY_MODEL_b9b91b22f462418c92e7d11a9090a0d0",
                "IPY_MODEL_2b6cf70d553f44939f3c6ae1855d37b0",
                "IPY_MODEL_4d1e1212643b4ad0b2ae9ef65e42f6f5"
              ],
              "layout": "IPY_MODEL_ee20ef7cadc14ecaa8d77c39feff6415",
              "tabbable": null,
              "tooltip": null
            }
          }
        },
        "version_major": 2,
        "version_minor": 0
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
