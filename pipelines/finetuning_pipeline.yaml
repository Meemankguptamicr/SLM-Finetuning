$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline

name: finetuning_pipeline
display_name: Finetuning Pipeline
description: Pipeline to fine-tune models, with potential for additional jobs.

settings:
  default_compute: azureml:cpu-compute

jobs:
  finetune_model:
    type: command
    component: ../components/finetuning/component.yaml
    inputs:
      run_name:
        type: string
        description: Name of the run.
        default: "raft_finetuning_demo"
      
      train_file:
        type: uri_file
        description: URI of the training data file.

      base_model_id:
        type: string
        description: ID of the base model to fine-tune.
        default: "microsoft/Phi-3-mini-128k-instruct"
      
      quantization_aware_training:
        type: boolean
        description: Enable quantization-aware training.
        default: true

      flash_attention:
        type: boolean
        description: Enable flash attention.
        default: true
      
      peft_approach:
        type: string
        description: Choose the PEFT approach (qlora, dora, lora).
        default: "dora"
        choices: ["qlora", "dora", "lora"]

      finetune_approach:
        type: string
        description: Choose the approach for fine-tuning a model (sfttrainer, unsloth).
        default: "sfttrainer"
        choices: ["sfttrainer", "unsloth"]
      
      optimizer:
        type: string
        description: Optimizer to use for training (adamw_8bit, adamw_torch_fused).
        default: "adamw_torch_fused"
        choices: ["adamw_8bit", "adamw_torch_fused"]

      max_seq_length:
        type: integer
        description: Maximum sequence length for the model.
        default: 4096
      
      num_train_epochs:
        type: integer
        description: Number of training epochs.
        default: 1
      
      learning_rate:
        type: number
        description: Learning rate for the optimizer.
        default: 1e-4

      per_device_train_batch_size:
        type: integer
        description: Batch size per device during training.
        default: 3
      
      per_device_eval_batch_size:
        type: integer
        description: Batch size per device during evaluation.
        default: 3
      
      gradient_accumulation_steps:
        type: integer
        description: Number of gradient accumulation steps.
        default: 6
      
      gradient_checkpointing:
        type: boolean
        description: Whether to use gradient checkpointing.
        default: true
      
      logging_steps:
        type: integer
        description: Number of steps between each logging.
        default: 10
      
      save_steps:
        type: integer
        description: Number of steps between each checkpoint save.
        default: 10
      
      eval_steps:
        type: integer
        description: Number of steps between each evaluation.
        default: 10

    outputs:
      model_dir:
        type: uri_folder